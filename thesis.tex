\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{adjustbox}
\title{Engineer's Thesis}
\author{Wojciech Pratkowiecki}
\date{December 2018}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{[PL] Konspekt}
\par
Całe badania zapoczątkowała praca naukowa autorstwa Y. Ganina i V. Lempitsky'ego pt. "Unsupervised Domain Adaptation by Backpropagation". Autorzy przedstawiają w niej Gradient Reversal Layer (GRL) - prostą sztuczkę znacznie ułatwiającą adaptację dziedzinową sieci neuronowej. O zadaniu adaptacij dziedzinowej możemy myśleć jako o próbie transformacji danych do reprezentacji będącej niewrażliwą na zmianę rozkładu danych, a skupiającej się jedynie na ich semantyce. Dla przykładu, jeżeli tworzymy model do rozpoznawania mowy, chcielibyśmy aby prawidłowo rozpoznawał on słowa niezależnie od mówcy tekstu. W ninejszej pracy skupiam się na rozpoznawaniu cyfr ze zdjęć. Istnieje wiele popularnych zbiorów z tego typu danymi - m.in. MNIST czy SVHN. Jednak aby dobrze rozpoznawać poszczególne z nich należy wytrenować sieć neuronową dla każdego przypadku z osobna. Chcielibyśmy zatem uzyskać model, który nie zwraca uwagi na to jaki jest charakter cyfr na zdjęciach, a skupiał się jedynie na tym, jaka cyfra jest przedstawiona na pojedynczym obrazku. 
\par
Ganin i Lempitsky próbują uzyskać reprezentajcę danych, która nie zawiera informacji o tym, z jakiego rozkładu pochodzą, a jedynie pozostawia te cechy, które nie zależą od dziedziny. W tym celu tworzą model złożony z trzech sieci neuronowych. Jedna z nich - feature extractor - ma za zadanie znalezienie odpowiedniej transformacji danych wejściowych do postaci możliwie podobnej zarówno dla dziedziny źródłowej jak i docelowej. Wówczas druga z sieci - predyktor klasy - uczy się odróżniania poszczególnych cyfr z takiej reprezentacji. Ostatnia sieć - predyktor domeny - próbuje odgadnąć z jakiego rozkładu pochodzi wejście stransformowane przez feature extractor i wymusza na feature extractorze zawarcie w reprezentacji wyjściowej danych jak najmniej informacji o dziedzinie z jakiej pochodzą. W tym celu predyktor domeny używa GRL - jest to bardzo prosta warstwa na samym początku sieci, która przy obliczaniu wyniku modelu zachowuje się jak identyczność przepuszczając przez siebie niezmienione dane, natomiast w czasie propagacji wstecznej mnoży gradient przez $-\lambda$, gdzie $\lambda$ rośnie nieliniowo na przedziale $(0,1)$ na przestrzeni treningu. W ten sposób feature extractor optymalizuje swoje parametry tak, aby błąd predyktora domeny był jak największy. Oznacza to, że reprezentacja danych jaką produkuje zawiera jak najmniej informacji o dzedzinie, co utrudnia zadanie predyktora domeny. Jednocześnie ta sama reprezentacja służy predyktorowi klasy do nauki rozróżniania liczb pochodzących tylko z rozkładu źródłowego. Jeżeli znajdzie on odpowiednie zależności dla tej postaci będziemy oczekiwać, że równie dobrze poradzi on sobie dla dziedziny docelowej, gdyż, zgodnie z ideą zasady działania predyktora domeny, jego dane wejściowe są niezależne od tego, z której dziedziny pochodzą. W ten sposób jesteśmy w stanie uzyskać znacznie bardziej uniwersalny model.
\par
Pierwszym etapem badań było zaimplementowanie architektury działającej na powyższej zasasdzie i próba odtworzenia wyników Ganina i Lempitsky'ego. Następnie należało zweryfikować dość wątpliwą tezę, mówiącą, że GRL sprzyja powstaniu reprezentacji danych, z której nie da się wywnioskować, z której dziedziny pochodzą. Architektura modelu sprawia, że otrzymujemy jedną sieć neuronową, która próbuje rozróżnić pochodzenie danych, jednak nie jest w stanie dobrze nauczyć się tego zadania, gdyż feature extractor ma za zadanie tej konkretnej sieci przeszkadzać. Jednakże ciężko jest stwierdzić, że skoro stworzyliśmy jedną sieć neuronową, która nie jest w stanie się wyszkolić w tym zadaniu, to dla każdej sieci neuronowej również nie uzyskamy skucesu. Kiedy więc spróbowałem wyszkolić nowy predyktor domeny na reprezentacji uzyskanej przez feature eztractor, który tym razem nie zawierał GRL, to już po kilku treningach skuteczność osiągnęła ponad 90\%. Widzimy zatem, że nasz model potrafi oszukać tylko jedną sieć neuronową, jednak wystacza to, do znacznego polepszenia wyników w klasyfikacji obrazków z dziedziny docelowej.
\par
Najnowsze i najskuteczniejsze prace na temat adaptacji dziedzinowej osiągają wyniki znacznie wyższe w przypadku klasyfikacji MNIST-M przy treningu na MNIST, na zbiorze testowym MNIST-M wynik potrafi przekraczać 90\%. Oznacza to, że wynik jaki ostiągamy przy użyciu GRL ma szanse polepszenia. Należy jednak zaznaczyć, że w większości prac na temat adaptacji dziedzinowej autorzy stosują pewnie przekształcenia na danych z dziedziny źródłowej. Poza popularnymi już obrotami czy dodawaniem szumów stosowane są metody takie jak dodanie do zbioru uczącego MNIST obrazków w negatywie - tło jest wówczas białe, a cyfra czarna, co z pewnością polepsza późniejszą klasyfikację obrazków kolorowych. We wszystkich moich eksperymantach nie stosowałem żadnych przekształceń danych. Początkowo bazowałem na przypadku adaptacji sieci uczonej dla MNIST do zbioru MNIST-M. W późniejszych etapach pracy przeprowadziłem kilka badań dla dziedziny źródłowej SVHN, a docelowej MNIST.
\par
Skoro GRL nie daje możliwie dobrego wyniku należało spróbwać usprawnić model. Kolejnym krokiem było przyjęcie bardziej matematycznego punktu widzenia. Feature extractor bierze oryginalne dane i mapuje je do pewnego $d$-elementowego wektora. Oznacza to, że jego wynik to punkt w przestrzeni $R^{D}$. Następnie próbujemy sprawić, aby te cechy danych w $R^{D}$, które mówią o przedstawianej przez obrazek wejściowy cyfrze, nie miały związku z dziedziną, z której pochodzą. Chcielibyśmy zatem, aby predyktory klasy i domeny korzystały z róznych zależności między punktami w $R^{D}$. Obie te sieci neuronowe posaidają warstwę ukrytą, która mapuje dane z $R^{D}$ do danych leżących w przestrzeni $R^{D'}$. Oznacza to, że spośród $D$ wektorów wzdłuż których leżą dane, obie sieci wybierają sobie po $D'$ z nich i rzutują na nie dane. Jeżeli sprawilibyśmy, aby kierunki te były od siebie zupełnie niezależne i faktycznie wyznaczały podział na klasy i dziedziny, to zmiana rozkładu danych nie wpływała by na wygląd wejściowy dla predyktora klasy, zatem ten poradziłby sobie równie dobrze. Kierunki są natomiast niezależne, gdy wektory je wyznaczające są do siebie prostopadłe. Zgodnie z ideą GRL po udanym treningu z jego użyciem macierze wymiarów $D \times D'$ predyktorów klasy i domeny, które wyznaczają $D'$ kierunków, z których korzysta dana sieć, powinny wyzmaczać wektory do siebie ortogonalne, zatem ich iloczyn skalarny powinien być bliski zeru. Kiedy spojrzymy na wagi sieci po treningu i policzymy średnią kwadratów iloczynów skalarnych tych macierzy, wynik jest faktycznie liczbą bliską zeru. Wydawać by się mogło, że wymuszenie jeszcze większej prostopadłości powinno polepszyć wynik modelu, który jeszcze lepiej odfiltrowywałby informację o wyniku. W związku z tym dodałem do funkcji straty całego modelu średnią kwadrtaów iloczyniu macierzy obu sieci i w ten sposób zmodyfikowany model zacząłem uczyć drugi raz. Dodatkowy składnik funkcji starty faktycznie sprawił, że iloczyn macierzy zaczął maleć, jednak nie polepszyło to wyników modelu. 
\par
Jednym z zagrożeń tego typu formuł w funkcji starty jest możliwość sytuacji, w której sieć zaczyna przypisywać parametrom bardzo małe wagi, zmniejszając długości wektorów rezprezentowanych przez macierze, w ten sposób zmniejszająć iloczyn macierzy. W naszym przypadku sytuacja ta nie miała miejsca, kiedy spojrzymy na średnie długości wektorów macierzy widać, że rosną one na przestrzeni obu treningów. Możemy jednak dodać do naszej funkcji straty jeszcze jeden składnik, dodający karę za zmainę oryginalnej długości wektora. W ten sposób trenując sieć po raz drugi spróbujemy zmniejszyć iloczyn skalarny macierzy predyktorów jednocześnie nie wydłużając wektorów przez te macierze reprezentowanych, zatem obniżenie wartości iloczynu powinno być łatwiejsze. Okazuje sie jednak, że ten trening nie ma wielkiego wpływu na wynik modelu, nadal jest on taki sam jak w architekturze nie używającej dodatkowych składników w funkcji straty.
\par
Postanowiłem sprzawdzić, czy minimalizacja długości wektrów macierzy faktycznie wpłynie negatywnie na trening. Nowa funkcja straty zawierała dodatkowe składniki, karzące za wielkość iloczynu macierzy a także za długości wektorów wyznaczonych przez macierze. Tak zmodyfikowana funkcja straty sprawiła, że zarówno długości wektorów jak i iloczyn macierzy bardzo szybko spadły w okolice zera. Zabieg ten nie wpłynął jednak negatywnie na wynik działania modelu, wręcz przeciwnie, na przestrzeni kolejnych treningów był on znacznie bardziej stabliny, jednak skuteczność klasyfikacji nie poprawiła się.
\par
Ganin i Lempitsky do optymalizacji parametrów swojego modelu używają Nestorov SGD z momentum. Jednak kiedy autorzy pisali arytkuł nie był jeszcze znany Adam optimizer, który z reguły radzi sobie najlepiej z doborem parametrów sieci neuronowej. Stworzyłem zatem model używający Adama. Po tak samo długim treningu jak w przypadku SGD skuteczność była delikatnie wyższa. Jednocześnie wartość iloczynu macierzy predyktorów klasy i domeny była znacznie wyższa niż w przypadku Stohastic Gradinet Descent. Z matematycznego punktu widzenia oznaczało to, że oba te predyktory korzystają z częściowo wspólnych informacji. Postanowiłem zatem ponownie rozszerzyć funkcję straty o karę za owy iloczyn. Przy koeljnym treningu wartość ilioczynu bardzo szybko spadła do okolic zera, jednocześnie nie skracając długości wektorów wyznaczonych przez macierze. Pomimo, wydawałoby się, zwiększenia separacji informacji o dziedzinie i o klasie, skuteczność modelu nie podniosła się. Nie mniej użycie Adam Optimizera wpłynęło pozytywnie na model.
\par
Kiedy matematyczna analiza modelu nie przyniosła polepszenia wyników, należało spróbować usprawnić model w inny sposób. Z dotychczasowych ekspreymentów wynika, że przy pomocy GRL uniemożliwiamy jednej sieci neuronowej znalezienia podziału na dziedzinę źródłową i docelową, jednak nadal jesteśmy w stanie wytrenować inną. W związku z tym po treningu z użyciem GRL postanowiłem najpierw wytrenować sieć, która potrafiła rozróżniać dziedziny danych stransformowanych przez feature extractor, a następnie dodałem na początek tej sieci neurowonej pojedynczą Grandient Reversal Layer. Zatem podczas drugiego treningu model, który nauczył się oszukiwać jedną sieć neuronową, miał za zadanie znaeźć reprezentację, na której inna, skuteczna sieć neuronowa przestanie działać dobrze. Po kilku iteracjach skuteczność nowego predyktora domeny, który cały czas próbował dostosować swoje parametry, spadła z ponad 90\% do poziomu 67\%. Ogólna skuteczność modelu nie poprawiła się, jednak sieć neurownowa znalazła nową reprezentację danych, która zawierała na tyle mało informacji o dziedzinie, że skuteczny na starcie model nie potrafił już rozróżnić pochodzenia danych. Po całym procesie spróbowałem uzyskać kolejny skuteczny predyktor domeny. Podobnie jak poprzednim razem, już po kilku iteracjach osiągnął on wysoką skuteczność. Pokazuje to, że GRL oszukuje pojedynczą sieć rozróżniającą dziedzinę, jednak nie znajduje reprezentacji zupełnie nieczułej na zmiany w rozkładzie wejściowym.
\par
Nieliniowa zmaina współczynnika $\lambda$ na przestrzeni całego treningu ma sprawić, że "oszukiwanie" predyktora domeny będzie dla feature extractora znaczącym zadaniem dopiero w późniejszym etapie nauki, najpierw skupiając się na odpowiednim kodowaniu danych dla predyktora klasy. Następnie, kiedy $\lambda$ zbliża się do 1 odfiltorwanie informacji o dziedzinie staje się ważniejszym zadaniem, przy jednoczesnym zapewnienu ciągle wysokiej skutecznośći predyktora klasy. Postanowiłem zatem wziąć wytrenowany model i ustawić dla niego stałą wartość $\lambda = 3$, a następnie wytrenować go ponownie. Po pierwszym treiningu ść sieci wynosiła około 79.2\%. Kidy zwiększyłem współczynnik $\lambda$ wymuszając na już wytrenowanym feature extractorze wymusiłem na nim przyłożenie jeszcze większej wagi to odfiltrowaniu z danych wejściowych informacji o dziedzinie. Po treningu z nową wartością $\lambda$ skuteczność modelu podskoczyła do 80.8\%. Jak widać udało się polepszyć działanie sieci, jednak nie należy zakładać, że ponowne wytrenowanie ze zwziększonym współczynnikiem $\lambda$ zawsze sprawi, że model będzie skuteczniejszy, gdyż sprawdziłem wynik modelu na treningu trwacjącym dwa razy dłużej - a zatem tak samo długo jak pierwszy trening ze zmienną $\lambda$ wraz z drugim, podczas którego $\lambda = 3$. Po dwuktornie dłuższym treningu skuteczność modelu również oscylowała w okolicach 81\%. Niemniej, jeżeli dojdziemy do ilości operacji treningu, której zwiększenie nie wpływa na poprawę działania modelu, dodatkowy trening ze zwiększoną wartością $\lambda$ wydaje się rozsądnym rozwiązaniem. Należy jednak rozsądnie dobrać nową wartość współczynnika, kiedy dobrałem wartość $\lambda = 6$ skuteczność modelu spadła, a trening stał się bardziej chaotyczny, gdyż feature extractor zbyt dużą wagę przywiązywał do oszukania predyktora domeny, co miało negatywny wpłym na reprezentację danych. 
\par
Skoro nie udało się polepszyć działania modelu przy ustalonej na początku architekturze, należało zastanowić się, czy nie byłaby wskazana drobna modyfikacja budowy modelu. Oryginalnie feature extractor mapuje dane wejściowe w przestrzeni $ R^{320} $. Próby wymuszenia odseparowania na 320 wymiarowym wektorze informacji o dziedzinie od cech odpowiedzialnych za cyfrę reprezentowaną przez obrazek nie udała się w poprzednich eksperymentach. Nalażało się wówczas zastanowić, czy 320 wartości na wektorze nie jest zbyt dużą ilością miejsca na informację, co pozwalałoby obu predyktorom poprawnie działać na tej reprezentacji. Być może mniejsza wymiarowość danych wejściowych dla predyktorów sprawiłaby, że nie pomieszczą się w niej informacje dla obu modeli i feature extracotr będzie musiał wybrać tylko te wartości, które pozwalają działać skutecznie klasyfikatorowi cyfr, nastomiast jakakolwiek informacja przydatna predyktorowi domeny nie zmieści się już w wyjściu feature extractora. Postanowiłem porównać wyniki dla architektur z feature extractorem mapującym wejście do przestrzeni ${320, 280, 240, 200, 180, 160, 120}$ wymiarowych. Po wytrenowaniu wszystkich modeli porównanie ich skuteczności nie przyniosło żadnych znaczących informacji, wszystkie radziły sobie dobrze na domienie źródłowej, na domenie docelowej zdecydowanie najlepiej radził sobie model z mapowaniem wejścia do $R^{320}$, najgorzej natomiast ostani z całej grupy, którego feature extracotr miał wyjście 120-wymiarowe. Zatem modele bardziej skompikowane znalazły skuteczniejsze parametry, co jest niemal regułą dla sieci neuronowych. Zatem zmniejszenie wymiaru do którego wstępnie mapujemy dane wcale nie sprzyja wyborowi przez feature extracotr jak najlepszych dla predyktora klasy cech.
\par
Jednym z kolejnych pomysłów do sprawdzenia było zmniejszenie wymiarowości wyjścia feature extracotra i brak GRL w predykotrze domeny. Stała za tym teoria, mówiąca, że jeżeli spróbujemy znaleźć mapowanie wejścia, na którym dwie różne sieci będą chciały działać jak najlepiej, to być może, przy odpowiednio małym wektorze wyjściowym, feature extracotr będzie zmuszony informacje kluczowe dla obu predyktorów upchać w osobne wartości wyjścia, aby zapewnić obu sieciom dobre działanie. Teoria ta została szybko obalona, dla nawet bardzo małej wymiarowości wyścia obie sieci miały wysokie skuteczności w okolicach 99\%, jednakże skuteczność klasyfikacji danych z dziedziny docelowej była znacznie niższa, niż w przypadku modelu trenowanego tylko na domenie źródłowej. Sieć neuronowa z 99\% skutecznością dla MNIST klasyfikuje MNIST-M w ok. 50\% prawidłowo. Kiedy wytrenowałem architekturę złożoną z feature extracotra oraz predyktora klasy i domeny bazujących na wyjściu z feature extractora, klasyfikator cyfr dla MNIST-M uzykał zaledwie 19\% skuteczności. Prowadzi to do teorii, że predyktor domeny wprowadził na tyle duże zaburzenia do feature extractora, że ten optymalizując swoje parametry podczas treningu bardzo mocno skupił się na zapewnieniu transofrmacji danych do postaci wygodnej dla predyktora domeny.
\par
Kiedy intuicyjne rozwiązania nie przyniosły rezultatu należało wstrzymać się z kolejnymi usprawnieniami i spróbwać zrozumieć, co tak naprawdę robi GRL. Do tej pory jedynym eksperymentem analizującym założenia GRL i intuicje na jego temat było douczenie predyktora domeny po treningu, co pokazywało, że GRL nie poelga na całkowitym wyfiltrowaniu informacji o dziedzinie przez feature extractor. Niemniej, częściowe usunięcie cech zależnych od dziedziny znacznie polepszyło skuteczność klasyfikatora na dziedzinie docelowej. Postanowiłem zbadać, jaka część informacji o dziedzinie przechodzi do predyktora klas, który w tym eksperymencie został rozszerzony do czterowarstwowej sieci neuronowej. W tym celu najpierw wytrenowałem model z tradycyjnym użyciem GRL jako pierwszej warstwy predyktora domeny. Po treningu skuteczność w klasyfikacji MNIST wynosiła 99\%, dla MNIST-M było to 78\%. Następnie spróbowałem uzyskać model, który z reprezentacji otrzymanej przez feature extractor wnioskowałby dziedzinę, z której pochodzi wejście. Skuteczność po 16 iteracjach wynosiła 95\%, co pokazuje częściowy zanik informacji o dziedzinie. Następnie spróbwałem uzyskać nowy predyktor domeny, tym razem bazujący na transformacji danych nie tylko przez feature extractor, ale także i przez pierwszą warstwę predyktora klasy. 16 iteracji pozwoliło uzyskać model ze skutecznością 88\%, co pokazuje, że faktycznie GRL pochłonął część informacji. Aby przez tę warstwę klasyfikatora przechodziło jeszcze mniej danych o dziedzinie podłączyłem w tym miejscu nowy predyktor domeny z GRL, jednocześnie zamrażając parametry feature extracotra, tak, aby predyktor klasy dalej bazował na tym samym wejściu, ale jego pierwsza warstwa próbowała jeszcze skuteczniej wybrać kluczowe informacje. 16 iteracyjny trening nieznacznie polepszył działanie ogólne modelu na dziedzinie docelowej. Kiedy kolejny predyktor domeny próbował wnioskować dziedzinę z nowej postaci pierwszej warstwy predyktora klasy jego skuteczność spadła do 75\%. Zatem udało się jeszcze bardziej ograniczyć znaczenie dziedziny po pierwszej warstwie predyktora klasy. Ten schemat działania - sprawdzanie ilości informacji o dziedzinie po kolejnej warstwie predyktora klasy, dodawanie do niej modelu z GRL i sprawdzanie, ile pozostało informacji o dziedzinie po zastosowaniu GRL powtarzałem aż do ostatniej warstwy sieci, której wyjściem było 10 liczb będących predykcją klasyfikatora. Wynik w tabelce X (będzie w ostatecznej wersji). Należy jednak zauważyć, że stopniowe zanikanie informacji o dziedzinie wraz z pogłębieniem się sieci wydaje się być naturalne, gdyż kolejne przekształcenia w warstwach modelu sprawiają, że dane stają się coraz bardziej zmodyfikowane, co może utrudniać znalezienie pewnych zależności z wejścia do sieci, których nie uczymy się explicite. Jako, że wynik klasyfikacji domeny docelowej na przestrzeni kolejnych podłączeń modelu z GRL ulegał delikatnemu poprawieniu, które może wynikać z coraz większego zanikania informacji o dziedzinie wydaje się, że ta metoda jest dobrą praktyką przy stosowaniu GRL w celach adaptacji dziedzinowej.
\par
Wszystkie dotychczasowe eksperymenty były przeprowadzone dla danych MNIST jako dziedziny źródłowej i MNIST-M jako dziedziny docelowej. Elementy z tych zbiorów, choć zdecydowanie różne, szczególne, gdy patrzymy na nie jako na macierze $28 \times 28 \times 3$, mają ze sobą sporo wspólnego - każdy obrazek z MNIST-M jest przekształceniem obrazka z MNIST. Aby zweryfikować otrzymane do tej pory wyniki postanowiłem zmienić dziedzinę źródłową na SVHN, a docelową na tradycyjnego MNIST. Skuteczny trening sieci neuronowej rozpoznającej cyfry ze zbioru SVHN wymaga znacznie bardziej skomplikowanej architektur feature extractora, sam trening również trwa znacznie dłużej. Po znalezieniu odpowiednich parametrów sieci udało się uzyskać model z około 92\% skutecznością dla zbioru testowego SVHN i 72\% udanych klasyfikacji dla MNIST, podczas, gdy bez używania GRL uzyskał skuteczność 94\% dla SVHN i 68\% dla MNIST. Tak wysoka skuteczność dla domeny docelowej (znacznie wyższa niż w pracy autorstwa Ganina i Lempitsky'ego) może wynikać z poziomu skomplikowania feature extracotra, który jest znacznie bardziej rozbudowany niż model używany przez autorów artykułu.
\par
Powtórzenie eksperymentów na SVHN nie przyniosło żadnych nowych obserwacji, ponownie wytrenowanie skutecznego predyktora domeny okazało się prostym zadaniem, wszelkie próby drugiego treningu wyuczonej sieci ze zmianą funkcji straty nie polepszyło wyników modelu. Podobnie jak w poprzednim przypadku informacja o domenie przechodzi do predyktora klasy, jadnak jesteśmy w stanię ją stopniowo odfiltrować, stabilizując nieco wyniki sieci. Przyjęcie SVHN jako dziedziny źródłowej wiąże się ze znacznie większym skomplikowaniem treningu, jednak przy odpowiedniej architekturze sieci jesteśmy w stanie odwzorować wyniki z przypadku adaptacji z MNIST do MNIST-M. Pokazuje to, że zależność między tymi zbiorami obrazków nie jest głównym powodem częściowego sukcesu w klasyfikacji cyfr. Użycie GRL jest zatem uzasadnionym i dobrym podejściem w próbie uzyskania bardziej uniwersalnego modelu.
\par
Skoro GRL częściowo działa i nie jesteśmy w stanie w trywialny sposób polepszyć jego rezultatów, warto spróbwać nieco dogłębniej zrozumieć jego działanie. Dane wejściowe dla naszej sieci to obrazki określonej wymiarowości ($28 \times 28 \times 3$ dla MNIST, $32 \times 32 \times 3$ dla SVHN), które transformujemy feature extractorem do D (głównie przyjmujemy $D=320$) wymiarowego wektora. W przypadku, gdy uczymy sieć na zbiorze MNIST feature extractor dokonuje zatem transformacji $R^{28 \cdot 28 \cdot 3} \rightarrow R^{320}$, stąd każdy obrazek możemy traktować jako punkt w przestrzeni $28 \cdot 28 \cdot 3 = 2352$ wymiarowej, który mapujemy na punkt w przestrzeni 320-wymiarowej. Następnie chcemy klasyfikować dane z dziedziny docelowej - MNIST-M. Każdy element z tego zbioru jest również obrazkiem o tych samych wymiarach co zdjęcia MNIST, zatem możemy je również traktować jako punkty w tej samej, 2352-wymiarowej, przestrzeni, które transofrmujemy do 320 wymiarów. Można zatem uznać, że zadaniem dobrego feature extractora byłoby mapowanie punktów z dwóch różnych rozkładów (MNIST, MNIST-M) do reprezentacji, w której chmury punktów utworzone przez obie dziedziny są możliwie podobne. Wówczas trudnym zadaniem byłoby rozpoznanie, czy dany punkt był oryginalnie przykładem z dziedziny źródłowej czy docelowej. Ponadto chcielibyśmy, żeby obrazki odpowiadające tym samym cyfrom były mapowane w te same regiony dla obu dziedzin.
\par
Jednym ze sposobów opisania chmury punktów jest wyznaczenie elipsoidy aproksymującej jej rozmieszczenie. Wprowadzone przez Herberta Jaegera konceptory bazują na tym właśnie pomyśle. Jaeger wyznacza elipsoidę przybliżającą chmurę punktów, której następnie używa do przekształcenia pewnego zadanego punktu na punkt leżący wewnątrz elipsoidy. W ten sposób chce on pomóc sieci neuronowej poprawnie zachować się dla wszystkich danych. Konceptor jest macierzą C wyznaczająca znormalizowaną elipsoidę leżącą wewnątrz unit sphere. Wektory własne macierzy C są kiernukami półosi elipsoidy, ich długości wyznaczają wartości własne C. Dla punktów leżących wewnątrz elipsoidy opisywanej przez konceptor, mnożenie przez macierz C ma zachować się jak mnożenie przez identyczność, dla punktów spoza tej elipsoidy przemnożenie powinno sprowadzic ten punkt wewnątrz niej. Pomysł ten wydaje się być dosć atrakcyjny w zadaniu adaptacji dziedzinowej. Jeżeli wyznaczylibyśmy koncptory opisujące chmury punktów uzyskane przez przekształcenie feature extractorem zbiorów punktów z dziedzin źródłowej i docelowej, moglibyśmy określić, jak podobne do siebie są obrazy obu dziedzin. 
\par
Jaeger wprowadza ponadto w swojej pracy miarę quota określającą, jak duża część przestrzeni jest zajęta przez konceptor. Jej wielkość wyznacza średnia arytmetyczna wartości własnych macierzy C. Ponadto wyznaczenie konceptora wymaga podania wielkości współczynnika przesłony, określającej jak duża część punktów danych wyznaczy elipsoidę. Im większa, tym późniejsze mapowanie punktów z zewnątrz ma większą ilość możliwych odwzorowań. W pracy naukowej "Overcomming catastrophic interference using conceptor-aided backpropagation" Xu He wraz z Jaegerem wyznaczają konceptory dla zbioru MNIST a także jego modyfikacji. W tej pracy ich celem jest wykorzystanie konceptorów do continual learningu, chcą, aby kolejne zadania dla sieci neuronowej korzystały z przestreni nie zajętej przez zadania już nauczone. Autorzy wykorzystują tu możliwość stosowania logiki I rzędu na konceptorach, gdzie ich suma wyznacza konceptor zawierający oba składniki sumy, iloczyn wyznacza część wspólną, a negacja dopełnienie przestrzeni elispoidy wyznaczającej konceptor. Dzięki temu autorzy wyznaczają elipsoidę określającą przestrzeń wymaganą przez pierwszy problem, a następnie wymuszają, aby następne zadanie korystało z przestrzeni poza tą elipsoidą. Kiedy już uzyskają model rozwiązujący nowy problem sumują konceptor określający przestrzeń tego modelu z konceptorem przestrzeni wszystkich poprzednich zadań, otrzymując opis przestrzeni zajmowanej przez wsztstkie zadnia. Quota konceptora dla wszyskich zadań określa, jak wiele sieć jest w stanie się jeszcze nauczyć, gdyż jej pojemność kończy się, gdy zajęta zostanie cała dostępna przestrzeń. Xu He i Jaeger w swoim artykule skupiają się na klasyfikacji obrazków ze zbioru MNIST, a także dziewięciu różnych permutacji pikseli tego zbioru. 
\par
Jako, że naszym problemem jest również opis przestrzeni wyznaczonej przez transformację MNIST, rozwiązanie Xu He i Jaegera wydaje się solidną podstawą dla naszego rozwiązania. Jednakże używają oni znacznie prostszej architektury sieci, tworząc model dwuwarstwowy bez warstw splotowych, co czyni go znacznie mniej skomplikowanym niż nasz. Ponadto nastawiony na użycie konceptorów trening ułatwia wyznaczanie elipsoid określających punkty o możliwie małej zajmowanej przestrzeni. W przypadku naszego modelu użycie wartości przesłony porównywalnej do wybranej przez autorów sprawia, że quota konceptora wynosi 99\%. Jesteśmy w stanie zredukować tę wartość znacznie zmniejszając przesłonę, jednak nie zmieni to faktu, że punkty wyznaczone przez przekształcenie feature extractorem elementów z dziedziny źródłowej są porozrzucane po całej przestrzeni i sensowne ograniczenie ich elipsoidą nie będącą kulą w 320 wymiarach nie jest możliwe. Sytuacja powtarza się dla przykładów z MNIST-M - dziedziny docelowej, zatem ciężko porównać nam podobieństwo chmur punktów wyznaczonych przez oba zbiory danych. Warto jednak dodać, że w przypadku, w którym nie używamy sieci klasyfikującej domenę, a feature extractor jest po prostu częścią architektury złożoną z warstw splotowych, quota konceptora dla dziedziny źródłowej ponownie szybko zbiega do 1, jednakże konceptor dla dziedziny docelowej znacznie wolnej zwiąksza wartość quota, co pokazuje ewentualną różnicę chmur punktów dla obu zbiorów, która sprawia, że wynik klasyfikacji dla domeny docelowej jest znacznie gorszy.
\par
Poznanie faktycznego rozkładu obu chmur punktów mogłoby pomóc w zrozumieniu działania zarówno GRL jak i powodu niepowodzeń przy użyciu konceptorów. Dodatkowo sama natura problemu adaptacji dziedzinowej mogłaby stać się znacznie bardziej zrozumiała. Z tego powodu postanowiliśmy w znacznym stopni ograniczyć wymiarowość D wyjścia feature extracotra, ustawiając $D=3$. Dzięki temu zabiegowi wejście do naszego modelu jest mapowane na 3 liczby, możemy zatem je zobrazować w 3 wymiarowym układzie współrzęnych. W ten sposób otrzymamy faktyczny wygląd chmury punktów wyznaczonej przez obie dziedziny po przekształceniu przez feature extracotr. Sekcja Visualization zawiera szczegółowy opis eksperymentu wraz z wynikami.
\par
Tak drastyczne zmniejszenie wymiarowości wyjścia feature extracotra odbija się negatywnie na wyniku adaptacji dziedzinowej, model z GRL uzyskuje gorze wyniki od tradycyjnej sieci klasyfikującej MNIST bez dostosowywania jej do innej dziedziny. Jednocześnie wizualizacje jakie umożliwia wartstwa rozmiaru 3 wydają się bardzo przydatnym narzędziem w analizie działania GRL. Aby nie rezygnować z możliwości wizualizacji danych, ale jednocześnie umożliwić feature extractorowi generowanie bardziej skomplikowanej reprezentacji danych, co również da większe pole do popisu GRL, możemy zmienić architekturę modelu, przywracając oryginalny feature extractor mapujący punkty wejściowe do wielowymiarowej przestrzeni $R^{D}$, a modyfikując predyktor klasy, dodając na jego początek warstwę transformujacą dane do 3 wymiarów. Dzięki temu nadal możemy wizualizować nasze dane poprzez przekształcenie obrazka wejściowego przez feature extractor oraz pierwszą warstwę klasyfikatora, jednocześnie umożliwiając GRL działanie z bardziej skomplikowaną reprezentacją danych. Model o tej budowie osiąga znacznie lepszy wynik w klasyfikowaniu danych z dziedziny docelowej - ma ok. 70\% skuteczności. Jednocześnie wizualizacje, jakie otrzymujemy przez naniesienie na układ współrzędnych nowej reprezentacji danych są bardzo podobne do tych, które otrzymaliśmy w poprzednim rozdziale.
\par
Otrzymane wizualizacje potwierdzają rozłożenie punktów w postać wieloramiennej gwiazdy. Aby potwierdzić nasze intuicje odnośnie przyczyn problemów w klasyfikacji części przykładów możemy sprawdzić, gdzie leżą punkty, które klasyfikujemy poprawnie z najwyższą pewnością, a gdzie te, co do których się mylimy. Wizualizacja pokazuje, że przykłady które sklasyfikowaliśmy źle są przez feature extractor wraz z pierwszą warstwą predyktora klasy mapowane w pobliże środka układu współrzędnych, gdzie wymieszane są ze sobą mapowania punktów z różnych klas, rzadko kiedy mylimy się co do punktów leżących odlegle od środka układu. Model jest natomiast najpewniejszy dla punktów położonych w ramionach otrzymanej gwiazdy, co wydaje się logicznym wynikiem. Dodatkowo możemy podjąć próbę zwizualizowania konceptora dla reprezentacji otrzymanej przez feature exrtactor. Jako, że jest on elipsoidą w wielowymiarowej przestrzni nie możemy tego zrobić wprost, jednkaże dobrym wyjściem wydaje się losowanie punktów w $R^{D}$ i sprawdzanie, czy leżą one wewnątrz elipsoidy. Następnie pierwszą warstwą predyktora klasy mapujemy do $R^{3}$ te punkty, które znajdowały się wewnątrz wyznaczonej przez konceptor elipsoidy. Dzięki zastosowaniu mniejszej sieci, a także wprowadzeniu regularyzacji, quota konceptora nie wynosi już 99\%, nasza elipsoida zajmuje teraz 85\% przestrzeni. Wizualizacja pokazuje, że punkty, które leżą wewnątrz konceptora układają się w elipsoidę i faktycznie nie wyglądają jak kula zajmująca całą przestrzeń. Jednakże próby zastosowania konceptorów w celu adaptacji dziedzinowej nie powiodły się, a same konceptory stały się dla nas niewiele bardziej zrozumiałe.
\par
Aby zwizualizować nasze dane nie jest konieczne używanie warstwy rozmiaru 3 w architekturze modelu. Z pomocą przychodzą nam różne metody redukcji wymiarowości, takie jak PCA. Metody te polegają głównie na rzutowaniu danych na pewne wektory własne macierzy wyznaczonej przez punkty danych. Aby uzyskać możliwie najlepszą redukcję wymiarowości wybieramy wektory własne odpowiadające największym wartościom własnym. Aby móc zwizualizować dane musimy ograniczyć nasze D wymiarów do zaledwie trzech. Jeżeli spojrzymy na wartości własne macierzy uzyskanej przez transformację zbioru wejściowego przez feature extractor zobaczymy, że trzy największe z nich nie odbiegają wielkością od kilku pozostałych. Oznacza to, że redukcja do3 wymiarów mocno zniekształca obraz danych. Jest to również potwierdzenie, że problem ten nie jest mozliwy do prostego rozwiązania z użyciem 3-wymiarowego wyjścia feature extracotra. Wizualizaje przy użyciu metod takich jak PCA czy KernelPCA nie są tak interesujące jak te z użyciem warstwy rozmiaru 3 wewnątrz sieci neuronowej, jadnak nadal pokazują, że użycie GRL pomaga w nałożeniu się na siebie chmur punktów dla obu dziedzin.
\par
Na zakończenie badań nad GRL przeanalizowałem faktyczny wpływ negacji gradientu na postęp w adaptacji dziedzinowej. W tym celu stworzyłem kilka innych modyfikacji wartości gradientu w czasie propagacji wstecznej. Jedną z nich było przemnożenie gradientu przez pewne losowe liczby z przedziału $(0,1)$. Operacja ta miała miejsce tuż przed przekazaniem wartości gradientu do feature extractora, który dostawał zatem wypaczone wielkości mówiące o wpływie poszczególnych wartości jego wyjścia na wynik działania modelu. Ten zabieg sprawił, że sieć ciągle dobrze klasyfikowała domenę źródłową, jednak jej wynik na domenie docelowej nie uległ polepszeniu. Jendoczeście predyktor domeny działał prawidłowo, rozróżniając oba zbiory danych, jednak tym razem nie miał negatywnego wpływu na klasyfikację dziedziny docelowej. Inną z modyfikacji gradientu było podmienienie go losowymi liczbami z przedziału $(-0.5, 0.5)$. Operacja ta spowodowała jednak, że funkcja straty przyjmowała w czasie treningu wartość NaN, co uniemożliwiało jakikolwiek treinig. Przy znacznym zminiejszeniu możliwego przedziału liczb losowych sieć uczyła się jak w przypadku, gdy nie posiadała dołączonego predyktora domeny, jednak wartości losowego gradientu były wówczas tak małe, że ciężko oczekiwać jakiegokolwiek ich wpływu na ostateczny wynik.
\par
Najciekawszą z modyfikacji gradientu była zmiana nie tylko znaku wartości, ale także odwrócenie jego wielkości. Mianowicie zamiast przenażania gradientu przez $-\lambda$ dokonywałem transofrmacji $g_{i} = (\max_{g \in G} g - g[i]) \cdot -\lambda$. Za taką operacją stoi idea, że skoro gradient mówi nam nie tylko o kierunku, w którym jest minimum, ale także o tym, które zmienne mają duży pływ na ostateczny wynik, to powinniśmy nie tylko iść w kierunku przeciwnym do minimum, aby predyktor domeny nie działał optymalnie, ale także sprawiać, by na jego wynik wpływ ciągle miały te zmienne, które nie rozstrzygają o wartościach jego wyniku. Taką modyfikację gradientu nazwałem Gradient Inversal Layer. Jej testy pokazują, że osiąga wyniki zbliżone do GRL, co potwierdza słuszność idei tej transformacji. Niemniej nie można jednoznacznie stwierdzić, czy sukces GIL nie wynika tylko i wyłącznie z sukcesu GRL. Nie wyklucza to jednak możliwości, że dokładniejsze przanalizowanie konceptu modyfikacji gradientu i wyznaczenie jeszcze innej funkcji transformującej, może przynieść wyniki lepsze od GRL.

\begin{abstract}
The amount of data that is still unlabeled has been growing over recent years. With domain adaptation methods it is possible to get a model that classifies well these samples, if only a related and labeled set is available. Therefore, the popularity of domain adaptation has strongly increased. While trying to get better and better results, many diverse and complex approaches were invented. This paper describes researches and experiments aimed at understanding the problem of domain adaptation and coping with it using the Gradient Reversal Layer.
\end{abstract}

\section{Introduction}
Neural networks have found many applications in recent years due to their ability of learning some complex data dependencies. Top performance in various tasks have resulted in increased popularity of not only neural networks and deep learning but also of other machine learning (ML) methods. Different architectures designed for different tasks were invented. One of them is convolutional neural network (CNN), which caused huge improvement in the image classification field. However, obtaining top-performing models often requires a long training, with huge amount of labeled data. Such a complicated procedure yields a desired model, but it would not reach comparable results on different, even strongly related dataset. Moreover, if we  have small amount of labeled samples, we need to find an alternative way to train the network. At this point domain adaptation is an attractive option.
\par
Domain adaptation (DA) is a process of learning a distribution of samples from source domain and obtain a model that performs well on other, but related, target domain. For instance, if we have a huge (e.g. synthetic), labeled set of road signs images and a video from a ride through a city, we could get an effective model that classifies well all the signs from the video frames, if we train it properly with the labeled dataset. One of domain adaptation techniques is the Gradient Reversal Layer (GRL) proposed by Yaroslav Ganin and Victor Lempitsky. GRL is a simple, but effective, architectural trick, applied during backpropagation, designed especially for domain adaptation task. This thesis is a collection of researches on the Gradient Reversal Layer and its adjustments. I tried not only to improve the results but also to get a detailed understanding of domain adaptation problem and GRL behaviour.

\section{Theoretical framework}
\subsection{Artificial neural network}

Artificial neural network (ANN) is a machine learning technique inspired by neural connections in human brain. While training the model we use a pre-defined architecture and loss function. The architecture describes the transformation of the input data made by our model, its complexity and interpretation of the output. Each network contains neurons that are stacked in layers. Neurons are nothing but a weighted sum characterized by neuron's weights and bias. A layer is built with many neurons and followed by non-linear activation function. Each layer transforms its input vector into an output vector. The first layer of the network is called the input layer, the last one is the output layer, while all the intermediate layers are called hidden layers. Figure~\ref{fig:ANN} presents a sample ANN. The loss function is a way to measure and optimize the performance of the model, it is commonly the magnitude of its imprecision. 
\par
Successful learning process requires many examples of the problem we want the network to solve. At the beginning, we usually set the model's parameters (mostly denoted as $\Theta$) to random values. The model transforms some of the input examples and we compute the loss function based on the output. As we want the model to perform as well as possible, our goal is to minimize the loss function. Therefore we use the backpropagation method - for each input sample we compute its gradient. The values of those gradients tell us, how should we tune model's parameters $\Theta$ to find the minimum of loss function. Therefore, we slightly change values of $\Theta$ in the direction of gradient, which should recall in a better performance of the model on the next examples. We repeat this process many times, improving our network during whole learning process. 
\par
This iterative training approach is a simplified description of stochastic gradient descent (SGD) - one of the most popular and effective ANN optimization method. As our model may be defined by millions of parameters, computational cost of thousands of training iterations is a huge drawback of neural networks. However, great growth of computer's computation power with GPU acceleration let us find solutions for some really complex problems within decent time.  Therefore neural networks are nowadays widely used for many tasks. One of them is image classification - finding for a given input image the most likely label from a fixed label set.

\subsection{Convolutional neural networks}
Applying neural networks to image processing became very popular. To keep reaching better results in tasks like image classification some new inventions were needed. The breakthrough came with convolutional neural networks (CNN). CNN, just like classical neural network, is built with neurons stacked in layers. First layers of CNN are convolution layers. Each of them is a set of learnable, small size (like 5$\times$5) filters followed by activation function. Each filter processes input image step by step and produces a transformed, usually smaller, one. Therefore after each layer we get a new set of images obtained from all pictures from previous layer. A big advantage of this approach is a much smaller number of learnable parameters - if the input image has size 1000 $\times$ 1000 it would require 1000000 parameters for a single neuron in a fully connected layer, while convolution layer processes it with its filter, that has 25 parameters, if the size of the filter is 5$\times$5.  After a convolution layer we usually use pooling - we divide the picture into cells (e.g. of size 2$\times$2) and we replace the whole cell with its maximum value (in case of maximum pooling). This operation reduces the size of the image internal representation, and force the network to focus on important patterns. Figure~\ref{fig:CNN} presents an example of CNN architecture.
\par
The convolutional part of the model learns high-level features or descriptors of locations in the input image. Moreover it makes the network invariant for small translation of object over the image. If our model classifies dog breeds, it will give the same result, no matter if the dog is in the middle or in a corner of the photo. Convolutional layers produce a vector representation of the input image. It is then fed into classical, fully-connected layers that return the model's output. If our task is image classification the output is a vector of classes probabilities typically.
\par
CNNs caused huge improvements in both accuracy and speed of image classification. Nowadays top-performing architectures in many image processing problems are built with convolutions. Convolutional layers also found application in some non-picture field like some natural language processing tasks or speech generation. 

\begin{figure}[htb]%
    \centering
    \includegraphics[width=\linewidth]{ANN.jpeg}%
    \caption{Example of neural network architecture}%
    \label{fig:ANN}%
\end{figure}
\begin{figure}[htb]%
    \centering
    \includegraphics[width=\linewidth]{CNN.png}%
    \caption{Example of convolutional neural network architecture}%
    \label{fig:CNN}%
\end{figure}

\subsection{Digit classification with CNN}
One of the most famous dataset in machine learning is MNIST - a collection of black and white photos of handwritten digits. Every picture's resolution is 28$\times$28 pixels, each one is an integer in the range $[0,255]$, that describes the brightness of the pixel. All the samples are provided with ground truth labels. Classification of digit images is therefore a task of processing the input image that dimensionality is 28$\times$28, and assigning probabilities for each of 0-9 digits. The model prediction is the digit with highest probability. Even very ordinary convolutional architecture reaches over 99\% accuracy on this dataset. A bit more challenging modification of MNIST is the MNIST-M set, obtained by blending example from MNIST with some color photos from BSDS500. There is probably no difference in difficulty of classification MNIST and MNIST-M digits for human, but from neural network point of view, the colorful set is much more complex. Nevertheless, obtaining top-performing model is relatively easy.
\par
There are a lot of similar digit recognition datasets, with many that are much more complex than MNIST and its modifications. The Street View House Number (SVHN) is a good example of such collection. SVHN samples are obtained from Google Street View photos, therefore they are colorful, with diverse backgrounds, pictures are often blurry and a single image may present few digits, while its label is the one in the centre of the photo. Due to all those properties of SVHN, model that classifies well its samples must be complex and longer trained. 
\par
Even though digit classification seems not to be the most important and entertaining problem that CNN may solve, it is still one of the most popular field of measuring a model's performance and yields experimental results with quick turn-around time. Figure~\ref{fig:digits} presents some examples from mentioned datasets - MNIST, MNIST-M and SVHN.

\begin{figure}%
    \centering
    \includegraphics[width=\linewidth]{digits.png}%
    \caption{Samples from MNIST, MNIST-M and SVHN digit images datasets, best viewed in color}
    \label{fig:digits}%
\end{figure}

\subsection{Domain adaptation}
Let's suppose that we have a novel, large set of digit images, for instance photos of the back of each European footballer's jersey, preprocessed in a way, that every image contains one digit - figure~\ref{fig:football} presents some possible samples. Our goal is to obtain a model that classifies digits from football kits. Unfortunately we don't have any labels for these pictures, so we are unable to go a straightforward way and train a CNN adjusted to the image set, as we cannot determinate if the model's prediction is right. In this kind of situation domain adaptation is a solution.
\par
Domain adaptation is a specific training process, when we teach a model with examples from a source domain, but our goal is to make it perform well on different, but related, target domain. Over recent years many architectures have been proposed to reach as high target domain accuracy as possible. DA is needed when samples from target distribution are unlabeled (unsupervised domain adaptation) or we have just few labeled examples (semi-supervised domain adaptation). A model should find a mapping between domains, which would allow source domain classifier perform well on test (target domain) examples.
\par
In our case of football jerseys, when the set is unlabeled at all, we have an unsupervised DA problem. Target domain is the distribution of jersey's photos. Now we should try to obtain a model learned with labeled examples from other dataset, like SVHN, and apply some architectural solution that would let the model classify well digits from football kits. Dozens of such tricks have been proposed in recent years. One of them is Gradient Reversal Layer.

\begin{figure}%
    \centering
    \includegraphics[width=\linewidth]{football.png}%
    \caption{Football jersey dataset}
    \label{fig:football}%
\end{figure}


\subsection{Gradient reversal layer}
Gradient Reversal Layer was introduced by Yaroslav Ganin and Victor Lempitsky in the paper titled "Unsupervised Domain Adaptation by Backpropagation" in 2015. The authors describe domain adaptation as a problem of finding common mapping for source and target distribution, that would transform samples to some kind of representation that allows classifier perform well and does not contain any information about the domain of mapped sample. In perfect scenario the mapping does not influence the label predictor's accuracy, but is so invariant w.r.t. domain shift that we cannot obtain a model, that predicts well the distribution of the input.
\par
Gradinet Reversal Layer itself is a simple trick used during backpropagation pass. It is a trival layer in network's architecture, that leaves the input unchanged during forward propagation, but multiplies the gradinet by negative scalar $\lambda$ during backpropagation. Formally we could say that GRL is a function specified as:
\begin{equation*}
GRL(x) = \begin{cases}
x &\text{forward pass}\\
-\lambda \cdot x &\text{backward pass}
\end{cases}
\end{equation*}
GRL demands a special architecture, contructed with three parts - a \textit{feature extractor}, that is denoted as $G_{f}$ and maps the input to a $D$-dimensional vector $f \in R^{D}$. Feature extractor is a neural network with its parameters $\Theta_{f}$, so the transformation of input vector $x$ is denoted as $G_{f}(x;\Theta_{f})$. This representation is then used by two other components of the model - a \textit{class predictor} $G_{y}$ with parameters $\Theta_{y}$, that classifies the $f = G_{f}(x;\Theta_{f})$ vector, 
and a \textit{domain predictor} $G_{d}$ with its parameters denoted as $\Theta_{d}$, which task is to predict the domain label of the vector $f$. The very first layer of $G_{d}$ is the GRL, therefore during backpropagation the gradient that comes to the feature extractor from domain predictor is multiplied by -$\lambda$. Figure~\ref{fig:GRLarch} presents a diagram with described architecture.
\par
All those three components are independent neural networks that construct our architecture. While training the model we aim to force $G_{f}$ to yield domain-invariant feature vectors $f$. GRL placed at the beginning of $G_{d}$ should be very helpful. The intuition behind inversion of the gradient sign during backpropagation is to follow the opposite direction to the domain predictor gradient, so after each update of the parameters of feature extractor, the feature vector $f$ should be more involved for $G_{d}$ and a good prediction of its domain label - much harder. $G_{d}$ itself tries to minimize its own loss function to make better predictions. If the domain predictor, that wants to does its best, is unable to determinate the distribution of the $f$, then $f$ is getting more domain-invariant. In perfect scenario GRL has such an impact on $\Theta_{f}$, that $G_{d}$ has the same accuracy as random model. Concurrently $G_{y}$ tries to predict a class of each sample and we try to minimize its loss function. Class predictor's gradient is used to tune $\Theta_{y}$ and then passed do feature extractor. Therefore the feature extractor's parameters are tuned w.r.t. gradient of $G_{y}$ and a negatively multiplied gradient of $G_{d}$. If we denote loss for class prediction as $L_{y}$ and loss for domain classification as $L_{d}$ then our goal is to minimize the function:
\begin{align*} 
E(\Theta_{f}, \Theta_{y}, \Theta_{d}) = &\sum_{\substack{i=1.N \\ d_{i}=0}} L_{y}(G_{y} (G_{f}(x_{i};\Theta_{f}); \Theta_{y} ), y_{i} ) - \\
 &\lambda \cdot \sum_{\substack{i=1.N \\ d_{i}=0,1}} L_{d}(G_{d} (G_{f}(x_{i};\Theta_{f}); \Theta_{d} ), d_{i} )
\end{align*}
where $(x_{i}, y_{i})$ are samples from $d_{i}$ and values $i$ of $d_{i}$ are 0 for source domain and 1 for target domain. Label predictor $G_{y}$ does not classify samples from target domain (as we don't have labels for them), so during the training this part of architecture doesn't even see any example from target distribution. Domain predictor of course process samples from both source and target distribution. The $\lambda$ coefficient grows non-linearly for 0 to 1 over the training, as we want the feature extractor to focus on good representation for class predictor firstly and making the vector $f$ domain-invariant when $G_{y}$ performs well already. 
\par
GRL is therefore a simple modification of model's architecture. Implementing it with some machine learning framework requires a little effort. If a model with GRL were creating absolutely domain-invariant representation of data the accuracy for test sets from source and target domain should be almost equal, as the only features left after $G_{f}$ transformation would be associated with the sample's class distribution. During tests we don't use domain predictor, our final model is build with feature extractor and class predictor, $G_{d}$ is used only to tune parameters of network during training.

\begin{figure}%
    \centering
    \includegraphics[width=\linewidth]{GRLarch.png}%
    \caption{Architecture with GRL in $G_{d}$ proposed by Ganin and Lempitsky - green part is a \textit{feature extractor} $G_{f}$ that transforms input sample $x$ to feature vector $f$. Afterwards it is classified by \textit{label predictor} $G_{y}$ (blue) and \textit{domain predictor} $G_{d}$ (pink)}
    \label{fig:GRLarch}%
\end{figure}

\section{Research}
\subsection{Paper reproduction}
First stage of all the researches made within the thesis was the paper reproduction. I made all implementations with PyTorch - deep learning python framework developed by Facebook. All computations were run on Google Colab notebook, as it provides a great computational power. During most experiments the source domain was MNIST set, while target distribution was MNIST-M. The biggest advanatage of such choice of domains is possibility of quick obtaining a trained model, as architecture needed to classify well MNIST images is not a complex one. I made few architectural changes in the network proposed by Ganin and Lempitsky, like replacing ReLU activation function with leaky-ReLU and using dropout during training. I managed to get a accuracy close to the result from paper (81.49\%) after 30 epochs of training, but already after 10 epochs the 77\% of samples from MNIST-M test set were classified well, while accuracy of domain predictor $G_{d}$ was 68\%. Therefore, a long learning process is not needed to verify if the training may be successful.
\par
When I managed to reproduce the paper, the next step was to verify if vector $f=G_{f}(x;\Theta_{f})$ really is domain invariant. When model's parameters were tuned a new domain predictor was constructed, this time without the gradient reversal layer. The input for new classifier was  vector $f = G_{f}(x;\Theta_{f})$ for all samples $x$ from both source and target domain, and the goal was to predict which distribution does $x$ come from. So far we knew that $G_{f}$ mapping was incomprehensible for a single domain predictor $G_{d}$ with parameters $\Theta_{d}$, as it's accuracy was just 68\%, while model that returns always 0 (or always 1) would reach 50\%. If the feature extractor mapping really is domain invariant we should not be able to obtain a model with much better performance.
\par
We could expect that in fact $G_{f}$ mapping is unclear for just one, used during training, domain predictor. After training the new classifier for just 4 epochs, it managed to predict well the domain of 94\% of examples from MNIST and MNIST-M mapped by $G_{f}$. Reaching higher accuracy is just a matter of epochs. Therefore we can certainly say, that representation $f$ of input image $x$ still contains a lot of information about the sample's domain, and getting a real domain-shift invariant representation requires much more than just gradient modification during the training. Nevertheless ability to outsmart a single domain predictor significantly improves the model's performance.

\subsection{Training continuations}
State of the art domain adaptation approaches reach much higher results than a model with GRL. Therefore looking for some improvement is not groundless. With MNIST as source domain and MNIST-M as target distribution, over 90\% accuracy on MNIST-M test set has been reached by most of recent architectures. It should be mentioned that top-performing domain adaptation models apply some data augmentation techniques, which make MNIST more similar to MNIST-M. One of such transformation is adding to the dataset a color-negative version of each sample (white background, black digit). During all my researches no augmentation was made, as in-depth understanding of the DA is more important than results boosting. However, it should be definitely checked if there is any possibility of improving the GRL.
\par
First attempt was based on some mathematical properties. As feature extractor $G_{f}$ maps input to vector $f \in R^{D}$ we can treat this vector as a point in D-dimensional space. We want representation $f$ of input image not to depend on its domain, that means, we want get to a situation where domain predictor $G_{d}$ requires some feature dependencies that are omitted by $G_{f}$ mapping, so the only features left are useful for the label predictor. Therefore we want $G_{y}$ and $G_{d}$ to use disjoint sets of data dependencies. Transformation made by each of classifier is in fact matrix multiplication. Predictors contain a hidden layer, that maps $f \in R^{D}$ to some $f_{h} \in R^{H}$. Moreover, multiplying the coordinates of D-dimensional point, represented by $f$, by some matrix is in fact projecting it onto some directions.  As both models take a point from $R^{D}$ as an input and process it based on some of its coordinates, we can say, that using disjoint dependencies is in fact transforming the input point with some pairwise (between predictors) orthogonal vectors. Then shifting domain means moving our data along directions that are used by domain predictor, which is unnoticeable for class predictor, therefore representation $f$ remains unchanged, as feature extractor ignore dependencies used by $G_{d}$.
\par
As mentioned before both class and domain predictors have a hidden layer inside. The hidden layer is a matrix $D \times H$ that determines H directions that matters for the model. We can then verify if predictors use orthogonal set of directions by computing the dot product of both matrices. If it is close to zero, the directions are more independent, so more orthogonal. When we compute the mean of square of dot product values after the model's training, it turns out to be a very small number. Therefore directions of $G_{y}$ and $G_{d}$ are almost orthogonal. It may be a good idea to use this learned model and train it again, with modified loss function, that would force $G_{y}$ and $G_{d}$ to minimize that dot product even more. We can add to the objective the mean of square of predictors matrices dot product, i.e. if we denote loss function as $E(\Theta_{f}, \Theta_{y}, \Theta_{d})$ and hidden layer matrices of classifiers as $W_{h_{y}}$ and $W_{h_{d}}$, then new loss function is:
\begin{equation*}
E'(\Theta_{f}, \Theta_{y}, \Theta_{d}) = E(\Theta_{f}, \Theta_{y}, \Theta_{d}) + \overline{(W_{h_{y}} \cdot W_{h_{d}}^{T})^{2}}
\end{equation*}
\par
The new loss function caused reducing of the dot product indeed, but it had no impact on model's performance. As new loss is punishing for product of model's weights the optimizer may force them to decay - lower values implies lower product. This would be undesirable, we want the dot product to reduce not because of shorter vectors, but because of their orthogonality. However monitoring the average length of these vectors shows that they are not shrinking. 
\par
Nonetheless we can add another component to the loss function, that would force $G_{y}$ and $G_{d}$ not only to make their hidden matrices orthogonal, but also to keep column lengths of these matrices unchanged. Once again, even the new loss function meets the expectations, model's performance doesn't improve.
\par
I decided to check if preventing the decay of these matrices weights is necessary, so instead of punishing for shrinking, the model was forced to to minimize the dot product of $W_{h_{y}}$ and $W_{h_{d}}$ as much as possible. When only the new loss with weight regularization was applied, the values quickly got close to zero. However it did not make the model performance worse, the accuracy over the epochs get even more stable. This experiment shows also, that weight decay usually is a good optimization practice.
\par
Extending the loss function in order to increase orthogonality of directions used by label and domain classifiers did not improve the model's performance. We could notice that before adding new components to loss function, these directions already were almost orthogonal, as GRL's task is to force the predictors to use disjoint features of their common input. As mathematical approaches did not enhance the network's result, some different attempt to improve the model should be done.
\subsection{Training continuation with increased $\lambda$}
The $\lambda$ coefficient grows from 0 to 1, as the feature extractor's output should be firstly adjusted to label predictor, and then made more and more domain invariant. When the final model is obtained, parameters $\Theta_{y}$ are well tuned, as accuracy on source domain - MNIST test set is about 99\%. $G_{y}$ is already well trained then, the only limitation is not domain invariant enough transformation made by $G_{f}$. As forcing this invariance is the $G_{d}$ aim, we could try to increase its impact on $G_{f}$ when $G_{y}$ is well trained. Therefore the new continuation of the learning process has a single modification - $\lambda$ has fixed value, that is greater than 1. In first attempt $\lambda$ was set to 3. After 10 epochs the model had 79.2\% accuracy on MNIST-M test set, for next 10 epochs of training fixed $\lambda$ was used, and the result increased to 80.8\%. If we train a fresh model for 20 epochs with increasing $\lambda$, we would reach comparable accuracy, so we cannot be sure, that the continued training with higher and fixed $\lambda$ will often improve the model's performance. However it seems reasonable to apply this kind of continuation, when larger epochs number does not help.
\par
Setting $\lambda$ parameter to some value grater then 1 is enhancing the impact of domain predictor on feature extractor mapping. However if the coefficient would be too large, it may lead to a decrease of label classifier accuracy. During experiments, when $\lambda = 6$ a pre-trained model gets chaotic, its result fluctuate over the epochs, and does not reach the level from first training. Therefore we should worry about picking the right value for $\lambda$ coefficient during training continuation.

\subsection{Adding GRL to well performing domain predictor}
As verified before, feature extractor produces representation that is indistinguishable for a single domain predictor. After the training we can easily obtain a model that successfully predicts the distribution of the sample, if only its architecture does not contain the GRL. We can check then, how difficult and effective would be changing the transformation made by $G_{f}$, so it become unclear for the well performing domain predictor, denoted as $G_{d}'$. We add the gradient reversal layer at the begining of the classifier's architecture. Then we train the whole model once again, but now the components are $G_{f}$, $G_{y}$ and $G_{d}'$. 
\par
After just few epochs accuracy of $G_{d}'$ has decreased significantly, as $G_{f}$ mapping adjusted to the new architecture. However, classifying samples from target distribution has not got any better. Also we don't know, how the $G_{d}'$ with GRL differs from $G_{d}$. If in fact both these predictors use highly correlated features to compute their results, then they are almost equal. Moreover, during training the whole model with $G_{d}'$, we don't use $G_{d}$ anymore, so we can't say, that $G_{f}$ produces the representation that is unclear for two domain predictors. Maintaining multiple domain classifiers may cause some improvement, however this approach would explain neither how GRL helps in DA task, nor what really is domain adaptation. Therefore it was not implemented.

\subsection{Adam Optimizer}
Stochastic gradinet descent (SGD) is probably the most popular optimizer used in deep learning. However, in recent years some fancy optimization methods released and they can boost the training process even more. One of them is Adam Optimizer. It is much more complex than SGD, with individual learning rate for each parameter and usage of more gradient properties. 
\par
To check if Adam may improve our model, the optimizer for each network was changed. After 10 epochs of training accuracy on target domain was slightly higher than before. The dot product of predictors hidden layers matrices was much higher than in the SGD case, therefore once again the modified loss function was used to minimize the dot product. The modification caused quick decay of this value, however the target domain performance did not change. Anyway, Adam optimizer turned out to be a good choice. Its parameters should be initialized carefully, as during the training the loss may get NaN value much easier. In our particular case, the initial learning rate was reduced from 0.01 for SGD to 0.001 for Adam.

\subsection{Tuning model size}
So far we did not manage to force the network to separate the information about the domain from some digit features. $G_{f}$ mapping satisfies both label and domain predictor, as we can easily obtain a successful domain classifier, so the representation $f=G_{f}(x;\Theta_{f})$ of input sample $x$ is not domain shift invariant. During all previous experiments, feature vector $f$ was 320-dimensional. Maybe the size is so large, that $G_{y}$ uses only its fraction, and the rest of the space is filled with information about the distribution of the input. Therefore a decreasing of $G_{f}$ output size should be considered.
\par
The intuition is that maybe there is an optimal size of feature extractor output. It would be big enough for $G_{y}$ to predict input's label with as high accuracy as with current architecture, but space left unused by class predictor would be too little to be sufficient for any domain predictor. The approach with a domain classifier without GRL should also be tried. If two models would compete for space of feature vector $f$, if it was too small, one of the network would "lose" and had no opportunity to learn.
\par
So far feature extractor was constructed with only convolution layers. During these experiments it was extended by single fully connected layer that was mapping the last convolutional layer's output from $R^{320}$ to $R^{D'}$, where $D' = 280, 240, 200, 180, 160, 120$. Then the smaller feature vector was classified by class predictor $G_{y}$ and domain predictor $G_{d}$ containing GRL. 
\par
When feature extractor tries to store all the features in lower dimension, the training process gets noisy. If the number of dimensions $D'$ is too little, value of the loss function may become NaN. It happens if the training lasts for too little epochs, when $\lambda$ gets close to 1 quickly and feature extractor is confused with two different gradients from $G_{y}$ and $G_{d}$. As $G_{f}$ returns just few values, both predictors compete for significant changes in most of them, what lead to the failure of the training. GRL may handicap the learning process then.
\par
After training each architecture the results were not surprising at all. The more complex the model was, the higher accuracy it reached. Moreover, adding fully connected layer caused noticeable fall of performance.  Figure~\ref{fig:size_tune} presents the accuracy of each model. 
\begin{figure}[htb]%
    \centering
    \includegraphics[width=\linewidth]{size_tune.png}%
    \caption{Accuracy of models with different feature extractor output size over the epochs. The \textbf{model 0} is the architecture used during previous experiments, with 320-dimensional $G_{f}$ output. \textbf{model 1 - model 6} are build with feature extractor that is extended with single fully connected layer, which return feature vector in smaller dimension, respectively: 280, 240, 200, 180, 160, 120. Labels present results on MNIST-M test set. We can see, that adding a fully connected layer caused decreasing of model's accuracy. }
    \label{fig:size_tune}%
\end{figure}

\par
As two different models use the feature extractor's output, $G_{f}$ tries to produce the result that would fit for both classifiers. So far GRL was jumbling it with purpose to make $G_{f}$ output clear for label predictor and as impenetrable as possible for domain classifier. If $G_{d}$ doesn't contain a gradient reversal layer, it would act like a regular binary classifier. The feature extractor would then try to return an output that would contain information about both the label of the input, as well as the domain of the sample. Therefore if the $G_{f}$ output would be too small, one of the classifier could receive too little data to learn well. If it would be the domain predictor, then the representation gets domain invariant.
\par
Once again feature extractor is extended by fully connected layer. This time it maps the feature vector to even 5-dimensional space. Our goal now is to train $G_{y}$ and $G_{d}$ to reach high accuracy simultaneously. When all the models are learned resluts are unsatisfactory. In each case both label and domain predictor perform well within their tasks - $G_{y}$ reaches over 98\% accuracy on MNIST test set, even when feature vector $f$ is 5-dimensional. Domain classifier in every architecture is almost always right. However there is an enormous decrease in target domain classifying performance - all the models have accuracy on MNIST-M test set under 20\%, twice less than a simple CNN trained on MNIST.
\par
Such a poor result definitely is a disappointment. Not only all the intuitions were wrong - limiting the output space did not force feature extractor to separate the data, but even model's performance get much worse. It seems that domain predictor's impact on feature extractor is so high, that the $G_{f}$ mapping becomes highly correlated with the distribution of the input, therefore, when we transform sample from target domain its representation is meaningless. Straightforward using of common feature vector mapping for two architectures is not a good idea then.

\subsection{Domain information vanishing}
Absence of domain information in feature vector is highly desirable in domain adaptation problem. Unfortunately GRL does not force such representation, as distribution predictor can be easily obtained - with 16 epochs of training the model achieved 95\% accuracy. Therefore, label classifier prediction is dependent on domain of the input image. Checking how successful can be a domain classifier on class predictor's first layer would show the magnitude of this dependency. 
\par
After 16 epochs of such classifier's training its accuracy in prediction the distribution of the input image was 88\%, then a decay of domain information happened, which improved the performance on target domain. Therefore, the growth of uncertainty of distribution classifier may cause even better results. This leads to the next experiment - "plugging" a domain predictor with GRL to the output of the first layer of class predictor. The weights of feature extractor are frozen now, as we don't want to change its mapping. The task now is to take the transformation, that leaves 88\% of information about the distribution of the input, and try to decrease this result with GRL.
\par
When the new domain classifier was applied to the output of $G_{y}$ first layer, after 16 epochs of training the accuracy on target domain test set was slightly better. To check if the distribution information had vanished even more, the new domain predictor was learned. Its accuaracy reached 75\%, therefore applying GRL to the output of the layer had definitely made in more domain invariant. This observation induces to check if the decay of distribution information would advance in deeper layers of the architecture.
\par
A special architecture of the network was used during thus experiment. Class predictor so far was built with an input layer, a single hidden layer and the output one. This time it was extended with an extra hidden layer. With deeper model we can monitor more precisely the decay of domain information. 
\par
After 16 epoch of training the accuracy of the model on the target domain (MINST-M) test set was 78.6\%. Then the feature extractor was frozen and the new distribution classifier (without GRL) was obtained to measure the domain information in $G_{f}$ output. Afterwards the first layer of $G_{y}$ was used just like feature extractor. The new $G_{d}$ with GRL was plugged to its output, therefore, the layer's mapping was used by the rest of class predictor and the domain classifier as well. This should lead to making the layer's output more domain invariant, what was checked with obtaining next distribution predictor. The procedure was then repeated on following layers of $G_{y}$. Table~\ref{tab:domain_vanishing} shows the results of predictors on each stage. Measuring domain information or applying model with GRL on $G_{y}$ output layer would be pointless, as these values are probabilities of each digit. Figure~\ref{fig:domain_v} presents the described learning process.

\begin{table}
\begin{center}
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth}
\begin{tabular}{c|c|c|c|c}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \\
Accuracy of \textbackslash{} model's layer & $G_{f}$ output  & input & ~1st hidden & 2nd hidden \\ 
\hline
Domain classifier on output & 99.99\%~ & 87.86\% & 75.13\% & 69.47\% \\ 
\hline
Domain classifier after adding GRL~ & 95.95\% & 74.75\% & 68.19\% & 68.6\%  \\ 
\hline
\begin{tabular}[c]{@{}c@{}}Target domain test set classification \\(after applying domain predictor with GRL)\end{tabular} & 78.6\% & 78.74\% & 79.06\% & 79.26\%    
\end{tabular}
\end{adjustbox}
\caption{Domain information vanishing research - for output layer of $G_{f}$ and input and both hidden layers of $G_{y}$ firstly the best possible domain predictor is obtained, then the $G_{d}$ with GRL is applied to the layer and accuracy of classifying the target domain test set and sample distribution are measured. While plugging model with GRL to one of the $G_{y}$ layers, the feature extractor and all previous layers of $G_{y}$ are frozen}
\label{tab:domain_vanishing}
\end{center}
\end{table}

\begin{figure}
    \centering
    \vspace*{-3cm}
    \includegraphics[scale=0.45]{domain_v.png}%
    \caption{Domain information vanishing process. Firstly the domain predictor is plugged to the output of $G_{f}$. Then $G_{f}$ is frozen and model with GRL is applied to the input layer of class predictor. In consecutive stages the layer is frozen, and GRL is plugged to the following one. Green blocks represent $G_{f}$, blue ones $G_{y}$ while pink are domain predictor $G_{d}$. Grey blocks are frozen layers. Best viewed in color.}
    \label{fig:domain_v}%    
\end{figure}

\par
Adding a domain predictor with GRL definitely caused the decay of distribution information, as applying it on $G_{y}$ input layer decreased the accuracy of domain classifier by 15\%. Therefore, the information passed by the first layer of class predictor is not as disturbed by distribution features as before. The experiment shows that applying model with GRL to layers of label classifier makes it more domain invariant. However, performance on test set from target distribution did not improve significantly, basic model with single domain predictor reaches similar result. Nonetheless it seems to be a good practise to apply GRL to more layers.
\par
Vanishing of domain information in deeper layers may be also caused by the complexity of input image transformation on this stage. The small decrease of domain predictor accuracy confirms the small impact of adding model with GRL. It seems like applying an extra domain classifier is more reasonable on early layers of $G_{y}$. It may be helpful observation, if the learning process lasts for a long time. 
\par
The deeper model was not as accurate as the one with single hidden layer in class predictor. Therefore, the experiment was repeated on the smaller model. After the training the accuracy on MNIST-M test set was 77\%. Domain predictor managed to classify well 94\% of examples from both distributions transformed by $G_{f}$. When the input data was also transformed by first layer of $G_{y}$ the domain classifier reached 89.69\%. Therefore, the input layer of class predictor also "filters" the distribution information.
\par
The training on each stage lasted 16 epochs. After applying model with GRL to the first layer of $G_{y}$ the domain classifier accuracy fell to 78.2\%, while performance on the MNIST-M test set increased to 80.6\%. The result of domain predictor on the hidden layer of the model was then 77.4\%. Afterwards classifier with GRL was once again applied, this time to the output of hidden layer of $G_{y}$. Plugging the model caused posterior decay of distribution information, the accuracy of new classifier reached 71.8\%. Simultaneously the classification of target domain test set improved to 80.7\%. Basic model trained for same number of epochs couldn't get higher than 80.57\%.
\par
Applying a new domain predictor with a gradient reversal layer turned out to be a successful modification. Although the improvement of results was small, the inner representations of input data are definitely more domain invariant, therefore it might be crucial in some situations. Also it came out, that if computational cost of applying new classifier to each layer is too high, the best practise is to use it with first layers of the label predictor.

\subsection{SVHN experiments}
All foregoing experiments were made when the source and target domains were MNIST and MNIST-M. Very high degree of relatedness between them may  be worrying, therefore the experiments made so far should be verified with some other datasets. One of the others digit image collections is the mentioned previously Street View House Numbers (SVHN) dataset. It was used as source domain, so the label classifier was learned with its samples. MNIST was a target distribution this time.
\par
SVHN is much more complex dataset, with many fuzzy images that presents more than one digit. Therefore obtaining well-performing model requires not only the longer training, but also more advanced architecture. The one used in following experiments had a feature extractor built with 5 convolutional layers, each one using batch normalization and the leaky-ReLU followed by max pooling and dropout during training process. The class and domian predictors have larger hidden layers than before. This architecture allows us to get reasonable results after about 18 epochs. The model proposed by Ganin and Lempitsky was not that complex, but they trained it for much longer, as for first 150 epochs classification error was high during their research.
\par
First training was a basic reproduction of GRL usage. The obtained model classified well 93\% of samples from SVHN test set and 76.5\% images from MNIST test set, slightly higher than best model obtained by the authors (71.07\%). Afterwards, the model built with just a feature extractor and class predictor was learned. The accuracy on SVHN test set was 94\% this time. Despite absence of a domain predictor with GRL, the model managed to recognize well 68\% of MNIST images. Such a well performance on target domain (Ganin and Lempitsky reached 59.19\%) is the result of the complex architecture.
\par
Just like in previous case, domain information is still available after $G_{f}$ mapping, we can easily get almost perfect distribution predictor. Also modifying the loss function with some mathematical theories did not improve the target domain performance. Applying the domain classifier with GRL succeed in partly removing the distribution information from $G_{y}$ layers, but did not increase the model accuracy on MNIST test set this time. Using Adam as optimization algorithm caused an improvement - not only the training was more stable, but the accuracy on target domain test set was 75\%.
\par
Repeating all the experiments with SVHN and MNIST as domains shows, that strong correlation between MNIST and MNIST-M was not the only reason of GRL partial success in domain adaptation. The improvement of the performance beside the model without domain predictor is then really caused by the ability of the network to focus on the semantic of the image - a digit. Obtaining domain-shift invariant representation of the input data does not happened with just GRL, but definitely such transformation would bring a huge triumph in DA.

\section{Gradient modifications}


\section{Conceptors}

\section{Visualization}
\par
To get some intuitions about how Gradient Reversal Layer and Conceptors work I tried to visualize the output of feature extractor. To do so I added an extra layer $G_{3}$ that takes from feature extractor $G_{f}$ the output vector ${ f } \in { R } ^ { D }$ and maps it to ${f'} \in {R}^{3}$, so the input vector ${x}$ is mapped into 3-dimensional space as $G_{3}(G_{f}(x))$. Now we can treat it as a 3-dimensional point. 

\subsection{MNIST and MNIST-M}
\par
Training model with such a small layer inside is much more difficult task, as many information may be lost during mapping do ${R}^{3}$. Therefore at the beginning I tried to get a simple MNIST classifier with no Gradient Reversal Layer. Fortunately the model achieved 98\% accuracy on the test set, but it performed poorly on MNIST-M - it managed to recognize just 31\% of digits, while without 3D mapping model's accuracy is 61\%. Now we can check how the input space ${X}$ looks after mapping by feature extractor. Figure ~\ref{fig:MNIST_3D}(a) shows the result of mapping for all samples from MNIST test set (blue) and MNIST-M test set (gray). Each point represents coordinates of vector obtained by passing it through feature extractor with 3D mapping, so the points cloud can be described as $F_{i} = \{G_{3}(G_{f}(x)) | x \in X_{test_{i}}\}$, where $i \in \{$MNIST, MNIST-M$\}$. The shape of ${F_{i}}$ resembles a multi-armed star. Moreover it seems like $F_{MNIST}$ is just a stretched version of $F_{MNIST-M}$. We can expect that each arm contains mapped pictures of same digit. As we can see in Figure ~\ref{fig:MNIST_3D}(b) zeros from both source and target distributions lie in the same direction and are represented by corresponding arm of stars. \par
Intuitively the domain adaptation may be seen as a task of finding a mapping for both source and target domain that yields the output which we can classify well, but we should be unable to tell the domain that the input sample comes from. In ours case we can assume that if we managed to extend our mapping with a mechanism that "stretches" the output for samples from target domain and leaves those from source distribution unchanged, we could get a classifier for both domains.

\begin{figure}[htb]%
    \centering
    \subfloat[MNIST and MNIST-M]{{\includegraphics[width=4cm]{MNIST_MNISTM.png} }}%
    \qquad
    \subfloat[Zeros of datasets]{{\includegraphics[width=4cm]{Zeros_MNIST_MNISTM.png} }}%
    \caption{The visualization of samples transformed by feature extractor with output size equals 3. (a) presents two stars formed by samples from source domain (MNIST/blue) and a target domain (MNIST-M/gray) (b) shows how pictures of zeros are mapped - yellow points are zeros from MNIST and green ones from MNIST-M}%
    \label{fig:MNIST_3D}%
\end{figure}

\subsubsection{Conceptors}
\par
Our first approach could be using of conceptor computed with samples from $F_{MNIST}$. As we assume, the conceptor defines an ellipsoid that approximates points cloud from certain distribution. Therefore multiplying some set of points by the concpetor matrix $C_{MNIST}$ would project the set into space of $F_{MNIST}$. Unfortunately in 3-dimensional case, the $C_{MNIST}$ is almost an identity matrix $I_{3}$. We should not be surprised, as our data points cloud is a mulit-armed star and limiting it with an ellipsoid requires it to be a sphere, we cannot approximate those points with any kind of flatten ellipsoid. Moreover, the singular value decomposition (SVD) of conceptor matrix $C$ results in $C = U\Sigma U^{T}$, where $U$ and $U^{T}$ can be interpreted as rotation matrices and $\Sigma$ is a diagonal matrix with singular values of $C$ on diagonal. As all singular values of conceptor matrix are real numbers from $[0,1]$, the conceptor matrix multiplying is in fact rotation, shortening (as non-zero values of $\Sigma$ are $\leq 1$) in certain directions and then re-rotating all the points (see Figure ~\ref{fig:ConceptorRotation}). Therefore we cannot transform $F_{MNIST-M}$ to resemble $F_{MNIST}$ with conceptor matrix $C_{MNIST}$, as the multi-amred star formed by points cloud would get even smaller, so it would increase the difference between source and target domains.

\begin{figure}[htb]%
    \centering
    \subfloat[$F_{M}$]{{\includegraphics[width=2.2cm]{C1.png} }}%
    \qquad
    \subfloat[$U^{T} \cdot F_{M}$]{{\includegraphics[width=2.2cm]{C2.png} }}%
    \qquad
    \subfloat[$\Sigma \cdot U^{T} \cdot F_{M}$]{{\includegraphics[width=2.2cm]{C3.png} }}%
    \qquad
    \subfloat[$U \cdot \Sigma \cdot U^{T} \cdot F_{M}$]{{\includegraphics[width=2.2cm]{C4.png} }}%
    \caption{The visualization of transformation points cloud by its conceptor based on SVD of concpetor matrix $C$. The representation $F_{MNIST}$ of MNIST test set (blue points) has firstly been rotated (gray), then scaled (yellow) and finally re-rotated (green)}%
    \label{fig:ConceptorRotation}%
\end{figure}
\par
With this kind of knowledge one thing we can do is transforming points from $F_{MNIST}$ with conceptor matrix $C_{MNIST-M}$ instead of transforming the $F_{MNIST-M}$ with $C_{MNIST}$. As mentioned before, the concpetor matrix is making points cloud tighter, so we could try to take the bigger points cloud (the one corresponding to MNIST) and fit it to the size and shape of the smaller one (see Figure ~\ref{fig:ConceptorMnist}). As we already have a well-trained model that performs well in MNIST recognition, we can extend it with a layer with no learnable parameters, that gets the input and multiplies it by conceptor matrix $C_{MNIST-M}$. Formally our model so far is a sequence of a feature extractor $G_{f}$ (with parameters $\Theta_{f}$), 3D mapping and its parameters $\Theta_{3}$ and label classifier $G_{y}$ with parameters $\Theta_{y}$, so the result for input vector $x$ is $G_{y}( G_{3}( G_{f}( x ; \Theta_{f}) ; \Theta_{3} ) ; \Theta_{y} )$. Now we want to freeze the trained feature extractor and 3D mapping (let's call this part of our architecture as $G_{3}$) and multiply its result by $C_{MNIST-M}$, what gives us the new representation ($G_{3} ^ {C} = C_{MNIST-M} \cdot G_{3}$) of the input.
\par
The mapping of points from target domain with $G_{3}^{C}$ compared to transforming source domian with $G_{3}$ is illustrated in the second picture of Figure ~\ref{fig:ConceptorMnist}. If the model constructed with non learnable $G_{3}^{C}$ and re-learned $G_{y}$ learns to classify the digits from source domain we would expect the model containing $G_{3}$ and $G_{y}$ perform well on target domain, as the $G_{3}$ transforms MNIST-M to grey points cloud from mentioned picture, so $G_{y}$ should hardly see the difference between domains. After just few epochs the label classifier adjusted to new representation and managed to reach 98\% accuracy on MNIST test set. Now we "unpack" the $G_{3}^{C}$ to $G_{3}$ and check its performance on MNIST-M. The model got 36\% of accuracy, so it improved by 16\% from previous 31\%. We can see the improvement, but we could expect something better, as we forced domains to get really similar.
\par
The last picture of Figure ~\ref{fig:ConceptorMnist} presents three points clouds - the blue one is the result of transforming source domain test set with $G_{3}C$. The yellow is target domain test set after transformation by $G_{3}$ (same as grey points from the second picture). The green points cloud is a mapping target domain by $G_{3}^{C}$. As conceptor matrix is making points cloud tighter this distribution was expected. It is worth mentioning that we can easily get a domain predictor $G_{d_{3}}$ that is performing well on both $G_{3}$ and $G_{3}C$ transformation - the accuracy of prediction whether the point belongs to target or source domain (equivalent to determinate if the point with given coordinates is blue or gray if we are considering distribution from first picture of Figure ~\ref{fig:ConceptorMnist}) reached 95\%.
\par
If we look closer at the plot of points we could see that many of target domain ones lie closely to the middle of coordinate system, so maybe the mapping works fine for just few examples which forms the multi-armed star, but most of samples are left with poor transformation, which is unclear for label classifier. Therefore after this test we can have few conclusions. Firstly we cannot use our conceptors intuitions from 3-dimensional space in $d$-dimensional one, as in 3D we are not able to approximate the multi-armed star obtained by samples with nothing but sphere. Nevertheless the improvement of predicting labels for target domain after including conceptor layer shows, that we should consider using conceptors during domain adaptation. The density of target domain points near the $(0,0,0)$ shows that those points clouds look similar at the first glance, but we cannot get a good classifier for target domain with this kind of representation. With this knowledge next things we can do to have a better understanding of our problem is trying to get a model with Gradient Reversal Layer in 3-dimensional space, and check the performance of conceptor layer in higher dimensions. Moreover we can check the variant with mapping feature extractor to 2-dimensions to verify all noticed behaviours. 

\begin{figure}[htb]%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=3cm]{Cm(X).png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{Cm(X)_Xm.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{Conceptor_train.png} }}%
    \caption{Transforming data with conceptor matrix $C_{MNIST-M}$ corresponding to target domain samples. First picture shows the 3D mapping of source domain (blue) and its transformation. The second one presents the transformation $G_{3}C$ of samples form source domain and 3D mapping $G_{3}$ of target domain test set. Last picture shows distributions after training label classifier on $G_{3}C$ mapping - blue points are again transformation of source domain, yellow ones are 3D mapping of target domain (equivalent to grey points from previous picture) and green points cloud is its $G_{3}C$ transformation.}%
    \label{fig:ConceptorMnist}%
\end{figure}

Before changing learning method we can see what happens when we change an activation function. All the plots shows that points cloud for both domains differ in size. One way to limit this magnitude is to use a $tanh$ activation function, which maps the input into $(-1, 1)$. Training model with 3-dimensional layer and $tanh$ activations is even more difficult, I managed to get 78\% accuracy on source domain test set, but it was performing better for target domain than model with $leaky ReLU$ (43\%).  As we can see in Figure ~\ref{fig:tanh} the mapped points formed something like a cube. The first plot presents the distribution of samples from source domain. Yellow points are locations of pictures of zero digit. We can see more those clusters on the edges of cube, so they should correspond to different digits. Second plot contains the distribution of target domain (gray). It is hard to find some pattern in them, they seem to be shuffled.

\begin{figure}[htb]%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=4.5cm]{tanh1.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{tanh2.png} }}%
    \caption{Changing activation function form $leaky ReLU$ to $tanh$}%
    \label{fig:tanh}%
\end{figure}

\subsubsection{Gradient Reversal Layer}
\par
We already know that GRL is a helpful tool in domain adaptation. In higher dimensions models with GRL achieved even 80\% accuracy. There are some approaches using Generative adversarial networks (GAN) that perform even better, so we can still improve our method. Therefore understanding of what really  GRL does is necessary to move on. We can use a model with 3-dimensional feature extractor output then and visualize the results. Now our architecture is not only feature extractor with 3D mapping $G_{3}$ and label predictor $G_{y}$, but also domain classifier $G_{d}$ that is using GRL during training so that representation yielded by $G_{3}$ does not contain information about domain. After a short training I got a model with 96\% accuracy on source domain test set and 40\% accuracy on target domain test set. The improvement is almost 30\% in relation to model without GRL and domain predictor. We can expect now that if we plot the $G_{3}$ mapping of points clouds for source and target domain we should see again two multi-armed stars but more similar than in previous section. 
\par
However the result is really surprising. We can see in Figure ~\ref{fig:GRL3D} how samples from source and target domain were transformed. First picture presents the distribution of source domain. It looks similar to the one obtained with model without GRL. The second picture is something we didn't expect - samples from our target domain - MNIST-M formed a blob that is not similar to the source domain distribution at all. When we look at the last picture of the figure we can see the location of samples with label zero. Those from target domain are focused around zeros form source domain, but the matching seems much worse than in model without GRL. Therefore, the reason why GRL is such an improvement becomes even more incomprehensible.

\begin{figure}[htb]%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D1.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D2.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D3.png} }}%
    \caption{Output of feature extractor mapping when GRL is used during training. First picture presents how source domain samples are transformed. On the second one there are points clouds for both source (blue) and target (gray) domain. Last picture shows how points with 0 label are distributed - yellow come from source domain while green are from MNIST-M}%
    \label{fig:GRL3D}%
\end{figure}
I decided to take a closer look at the training. During the second run model reached even 52\% accuracy on target domain's test set. This result was obtained after second epoch of training, after the last one the model was able to distinguish 47\% of MNIST-M digits. Such a big difference in performance between two runs of the same training for the same architecture shows that with 3-dimensional layer model becomes very unpredictable. I ran the training (for a new instance of architecture) few more times, during third run model reached 47\% accuracy after first epoch but it ends with just 35\%. During next training the loss became NaN so the model crashed. After that I got a model with 55\% accuracy after 5-th epoch out of 10. Next runs also produced very diverse results. Result for source domain kept high during all of them. Figure ~\ref{fig:GRL3D_2} presents some results. First two pictures shows the distribution of points for model from second run - the one that reached 52\% accuracy during training and ended with 47\%. We can see that the best model from the training produced the representation closest to the distribution of source domain samples. After whole training the mapping is again very chaotic and noisy. The last picture is a plot from the training with 55\% best and 52\% final accuracy. Although the results are better the points clouds seem to be less suited to the source domain distribution. 

\begin{figure}%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D_B1.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D_B2.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{GRL_3D_B3.png} }}%
    \caption{Model with GRL during other runs. First two pictures shows plots for model with 52\% accuracy during training and 47\% after last epoch. Yellow points are the final mapping of target domain, grey ones are obtained with feature extractor from model with best accuracy. Blue ones are transformed samples from source domain. Last picture is a corresponding plot for the best obtained model.}%
    \label{fig:GRL3D_2}%
\end{figure}
\par
Every training reached its best accuracy after one of the firsts epochs, that are characterized with lower $\lambda$ coefficient. As $\lambda$ changes nonlinearly from 0 to 1 I decided to use fixed value $\lambda$ during the training. The coefficient was set to 0.3, 0.4 and 0.5. All the models did not managed to get a better performance than already obtained. The accuracy within a single training was also unstable, so fixing $\lambda$ did not make an impact on the research. We are able to make over 50\% improvement with GRL then, but we cannot tell clearly why it works. Figure ~\ref{fig:GRL_digits} shows how particular digits were mapped by feature extractor from model with over 50\% accuracy on MNIST-M test set. Each arm of star obtained from source domain samples corresponds to individual digit. In blob formed by target domain test set we can see a cluster for each digit, but it still looks like shuffled. Last picture presents the location of points for both domain in coordinate system. It seems that some of them overlap, what may cause better performance of a classifier.

\begin{figure}%
    \centering
    \subfloat[MNIST mapped]{{\includegraphics[width=3cm]{GRL_digits1.png} }}%
    \qquad
    \subfloat[MNIST-M mapped]{{\includegraphics[width=3cm]{GRL_digits2.png} }}%
    \qquad
    \subfloat[Both points clouds]{{\includegraphics[width=3cm]{GRL_digits3.png} }}%
    \caption{Mapping for particular digits. The points clouds are obtained with feature extractor of model that uses GRL during training. Last picture shows how both points cloud are located in 3-dimensional coordinate system.}%
    \label{fig:GRL_digits}%
\end{figure}
\subsubsection{2-dimensional case}
\par
Visualization with digit division is a good way to understand what really happens during training. To make plots even more understandable we can change our feature extractor to map output into 2-dimensional space. Although finding right parameters gets more difficult, after few epochs of training we can get a model with 95\% accuracy for source domain (MNIST) and 31\% for samples from MNIST-M. These results are satisfying as we are asking our model to pack all the crucial information about 28x28 pixel photo into 2 real numbers. First two pictures of figure ~\ref{fig:2D} presents the transformation of MNIST and MNIST-M test sets. Each point is a mapping of input image with digit corresponding to point's color. MNIST samples are formed into 7-armed star, each for different digit. Input images for 3 digits (3,5,8 in this case) are stacked in the middle, which does not interfere with good classifier performance. Samples from target distribution also formed similar 7-armed star but, like mentioned before, a large number of them was mapped near the middle of coordinates system. This characteristic is more clearly visible in 2-dimensional case. As input images for different digits are represented by similar numbers the classifier cannot be sure while predicting the label.
\par
We can also apply a training with Gradient Reversal Layer for this architecture. Just like before the source domain samples are mapped into multi-armed star and visualization of points cloud obtained by mapping the target domain resluts with a plot of huge blob (last picture of figure ~\ref{fig:2D}). The performance of this model is better thanks to the use of a GRL, but I could not get any new useful knowledge from this research. Also trying to improve the model's performance with conceptor did not succeed. As in the 3-dimensional case the conceptor of our mapped set of points is an ellipse that is limiting the points cloud. Here we can see that it would be difficult to approximate our samples by something but a circle, hence the conceptor matrix is almost an identity matrix. Therefore filtering the point by the matrix hardly changes its coordinates. Once again I must point out that we cannot be sure taht this behaviour of conceptor would happen in higher dimension.

\begin{figure}[htb]%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=3cm]{MNIST_2D.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{MNIST_M_2D.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{MNIST_M_2D_GRL.png} }}%
    \caption{Mapping for particular digits with feature extractor yielding 2-dimensional vector. First picture shows the distribution for samples from source domain, second one for input from target domain and the last one is a mapping of target domain samples after training with GRL applied.}%
    \label{fig:2D}%
\end{figure}

\subsection{SVHN and MNIST}
\par
All previous observations were made for highly correlated source and target domain, as MNIST-M is obtained with MNIST dataset. To be sure that results of the research is not caused by the connection between MNIST and MNIST-M we should try to run the experiments for some other domain. Once again I used SVHN dataset as a source domain and tried to improve its performance in MNIST digit recognition. 
\subsubsection{Model without Gradient Reversal Layer}
\par
Firstly let's try to get a model with 3-dimensional layer, that is performing well on SVHN classification. As predicting the label for samples from SVHN dataset is a way harder task than MNIST recognition the model must be more complicated and the task of learning architecture witch such a small layer inside may failed. Fortunately model with a complex feature extractor that was used previously to tune parameters for SVHN classifier succeed in learning task. After few epochs it reached 92\% accuracy on source domain (SVHN) test set and 65\% on target domain (MNIST) samples. The performance on MNIST test set is satisfying, it seems that we did not lose many information during mapping a vector transformed by feature extractor to 3-dimensional space, as without this layer we can get a model with just slightly better results (94\% accuracy for source domain, 70\% for the target one). Now let's look at the visualization of mapping a set of samples from both domains. As in previous section I used MNIST test set $MNIST_{T}$ (never seen by any architecture neither with GRL nor without) and $SVHN_{T}$ - 10000 randomly sampled images from SVHN test set. The feature extractor with 3D mapping is denoted again as $G_{3}$ - note that it is much more complex than $G_{3}$ from previous section. The points clouds obtained by mapping those input vectors sets are $F_{SVHN} = \{ G_{3}(x) | x \in SVHN_{T} \}$  and $F_{MNIST} = \{ G_{3}(x) | x \in MNIST_{T} \}$.   

\begin{figure}%
    \centering
    \subfloat[SVHN]{{\includegraphics[width=3cm]{SVHN.png} }}%
    \qquad
    \subfloat[SVHN and MNIST]{{\includegraphics[width=3cm]{SVHN_and_MNIST.png} }}%
    \qquad
    \subfloat[Zeros of datasets]{{\includegraphics[width=3cm]{Zeros_SVHN_MNIST.png} }}%
    \caption{The visualization of samples transformed by feature extractor with output size equals 3. (a) presents the star formed with samples from source domain (SVHN), (b) shows how data from target domain is mapped and (c) where zeros for both domains lie in the space}%
    \label{fig:SVHN_3D}%
\end{figure}
\par
First picture of figure ~\ref{fig:SVHN_3D} presents the distribution of $F_{SVHN}$. The samples from SVHN domain formed a solid resembling a multi-armed star, that looks much more sharper than the one obtained in previous section, when model was trained on MNIST dataset. Second image shows the relation between $F_{SVHN}$ (blue) and $F_{MNIST}$ (gray). Samples from target domain are formed in a very similar multi-armed star, even the magnitude of the solid is close to the one composed of SVHN input images. In the previous MNIST/MNIST-M section, the improvement of the model was correlated with reducing the difference in size of obtained points clouds, therefore the good performance of classifier on target domain is understandable. Last picture of the figure shows the positions of images of zero digit from both domains. For source domain our $G_{3}$ mapping found a way to form every digit into individual arm of the star. When we transform zeros from $MNIST_{T}$ with this mapping we can see that they are scattered over two arms of obtained star, also many of them lie in the middle of the solid. It causes the label prediction imprecission, as samples with different labels are mixed. Figure ~\ref{fig:SVHN_Digits} shows how every of 10 digits was distributed by $G_{3}$. Samples from source domain are well-formed in distinguishable arms of our star. In the target domain case we can also assign every arm of obtained solid to different label, but we can see that some of samples lie far from cluster formed by images with the same digit.

\begin{figure}[htb]%
    \centering
    \subfloat[$F_{SVHN}$]{{\includegraphics[width=4.5cm]{SVHN_digits1.png} }}%
    \qquad
    \subfloat[$F_{MNIST}$]{{\includegraphics[width=4.5cm]{SVHN_digits2.png} }}%
    \caption{Distribution of particular digit images after $G_{3}$ mapping.}%
    \label{fig:SVHN_Digits}%
\end{figure}

\subsubsection{Model with Gradient Reversal Layer}
\par
We already know that training a model with a Gradient Reversal Layer and 3-dimensional layer is much more complicated. Although I managed to get a well-tuned network in previous section, when I added a domain predictor $G_{d}$ with GRL inside I was unable to train a newly initialized model, during whole training the loss did not reduce so accuracy on test sets of both source and target domain was the same as for random model. This shows that the task became too complex, no model was able to pack all the crucial information in just three numbers while GRL was used. 
\par
One thing that came to my mind was to get the pre-trained model that did not use GRL, like the one from previous subsection, and add the domain predictor with GRL to it. This approach was partly successful, as after third epoch of new training (with GRL) the model reached 72\% accuracy on $MNIST_{T}$. Unfortunately at the begining of next epoch the loss value became NaN, so the model got useless. As the $\lambda$ coefficient is getting larger over the epochs I decided to fix it to a certain, lower value during whole training. With $\lambda$ set to 0.4 and 0.2 the pre-trained model improved in MNIST recognition to 73\%, so the GRL improved the performance by 12\%. The result is quite good, as without limiting the size of any layer I did not get much better. Figure ~\ref{fig:SVHN_GRL} presents the distribution of $F_{MNIST}$ after training the model with Gradient Reversal Layer. Distribution from first picture was obtained with model that tuned $\lambda$ during training, the parameters of used $G_{3}$ were serialized before the loss got NaN. Second picture is plot of transformation $MNIST_{T}$ with model that used $\lambda = 0.2$ during whole training. The result for $\lambda = 0.4$ is very similar, so there is no point in including the plot. In both pictures we can see that there are less points located in wrong arm of the star. This obviously leads to the better performance of the classifier. Unlike a MNIST/MNIST-M training case the obtained points cloud is recognizing the shape from no-GRL training. However in previos section we trained the model from the begining, here we use a pre-trained one and the $\lambda$ is set to a low value, so the mechanism of Gradient Reversal Layer has less impact on transformation of input vector.
\par
You can wonder if the multi-armed star look of samples from target domain is really caused by the relation between domains - both are images of digits, or if the shape has nothing to do with the domain shift and all those observations are pointless. To verify it I created a random dataset with samples of size equal to the size on SVHN image and values distributed form $\mathcal{N}(\mu_{SVHN},\,\sigma_{SVHN}^{2})$, where $\mu_{SVHN}$ is just the values' of pixels mean SVHN images and $sigma_{SVHN}^{2}$ their variance. The visualization of mapping this dataset with $G_{3}$ was just a blob of points, so we cannot suppose that the multi-armed star is not related with images of digits.

\begin{figure}[htb]%
\captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[]{{\includegraphics[width=4.5cm]{SVHN_GRL.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{SVHN_GRL2.png} }}%
    \caption{Distribution of particular digit images after $G_{3}$ mapping with GRL used during training.}%
    \label{fig:SVHN_GRL}%
\end{figure}

\subsection{Higher dimension performance}
\par
During studying the dependence between MNIST and MNIST-M we could see that adding the conceptor computed for target domain to our feature extractor and then re-tuning the label predictor can improve the result of classifier for target samples, as the new classifier has its input more related with target domain distribution. To check if this behavior occurs in higher dimension I trained the model without 3-dimensional layer and ten add the conceptor matrix $C$ multiplication to the feature extractor $G_{f}$ output. Then the label predictor $G_{y}$ was trying to predict the digit for representation $G_{f}^{C}(x)$ of input vector $x$. Just like in 3-dimensional case there was no problem for $G_{y}$ to adjust to the new input distribution. After the training I unpacked $G_{f}^{C}$ to $G_{f}$ and checked the performance of the model. In contrast to the model with 3-dimensional layer inside, this time there was no improvement on target domain test set. This results show that it is hard to get the intuition about how conceptors really word with 3-dimensional mapping of such complex data.

\end{document}
